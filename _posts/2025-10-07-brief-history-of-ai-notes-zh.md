---
layout    : post
title     : "[笔记]《人工智能简史（第二版）》（2025）"
date      : 2025-10-07
lastupdate: 2025-10-07
categories: ai
---

尼克的《人工智能简史（第二版）》从人和流派传承的角度介绍了人工智能作为计算科学一个分支的发展过程，
内容和风格有点偏学术史，用作者的话说，“写法比较偏重基础和方法论，而不太注重应用”。
作为一本不太“常规”的人工智能入门读物，适合领域内的部分专业读者，
或者想从科学、哲学、伦理学等更高角度理解和看待人工智能的读者。

<p align="center"><img src="/assets/img/brief-history-of-ai-zh/brief-history-of-ai-book-cover.jpg" width="35%" height="35%"></p>

本文整理一些个人阅读笔记和思考。

水平及维护精力所限，文中不免存在错误或过时之处，请酌情参考。
**<mark>传播知识，尊重劳动，年满十八周岁，转载请注明<a href="https://arthurchiao.art">出处</a></mark>**。

----

* TOC
{:toc}

----

# 0 前言

## 0.1 哈代：一等智力 vs. 二等智力

哈代曾说**<mark>科学和艺术的原创需要一等的智力，解释和欣赏</mark>**（例如乐评家和书评家）是**<mark>二等智力</mark>**的活儿。

搜了一下哈代的原话：

> It is a melancholy experience for a professional mathematician to find
> himself writing about mathematics. The function of a mathematician is to do
> something, to prove new theorems, to add to mathematics, and not to talk
> about what he or other mathematicians have done. Statesmen despise publicists,
> painters despise art-critics, and physiologists, physicists, or
> mathematicians have usually similar feelings; **<mark><code>there is no scorn more profound,
> or on the whole more justifiable, than that of the men who make for the men
> who explain. Exposition, criticism, appreciation, is work for second-rate
> minds</code></mark>**.
>
> A Mathematician’s Apology，G. H. Hardy

大致意思：

> **<mark>让一个职业数学家花时间去阐释数学相关的东西是悲哀的</mark>**。
> 数学家的本职工作是创新，例如证明新定理，发现新东西，而不是去宣讲自己或其它数学家做了些什么。
> **<mark>政客鄙视政治评论家，画家鄙视艺术评论者</mark>**，
> 生理学家、物理学家或数学家通常都有类似的感受。
> **<mark>没有任何嘲笑，能比创造者对解释者的嘲笑来得更深远，或在整体上更为合理</mark>**。
> 阐释、批评、欣赏，都是二等智力者的工作。
>
> 《一个数学家的自白》，哈代

## 0.2 任正非

> 任正非是二十一世纪的哈代。
>
> 我自己日暮黄昏，但任正非只七十四岁，来日方长。我希望任先生不要管他人怎样说，
> 因为哈代说得清楚，“没有任何嘲笑能比创作者对解释者的嘲笑来得更深奥，或在整体上更为合理。
> 阐释、批评、欣赏，都是只有二等脑子的人的工作。”
>
> [张五常：任正非是今天的哈代吗](https://m.huxiu.com/article/282689.html), 2019

# 1 达特茅斯会议：人工智能的起源， 1956

> **<mark><code>What is past is prologue</code></mark>**. - William Shakespeare
>
> 凡过往皆为序章。

## 1.1 经典读物

1. "Man viewed as a Machine" 介绍了图灵机和冯诺依曼的细胞自动机。

    * muscle machine
    * brain machine - 人工智能的另一种说法

2. [Alchemy and Artificial Intelligence (PDF)](https://apps.dtic.mil/sti/trecms/pdf/AD0625719.pdf),《炼金术与人工智能》，1965
3. 《计算机不能干什么》
4. 《Human Memory and the Storage of Information》1956

    是《The Magic Number Seven》的另一个版本。

一门年轻的学科，一开始都需要一点“过度销售”（excessive salesmanship） - Minsky

## 1.2 Chomsky：机器可以思考吗？-> 潜艇会游泳吗？

2015 年他被问及“**<mark>机器可以思考吗？</mark>**”，
他套用计算机科学家 Dijkstra 的说法反问：“**<mark>潜艇会游泳吗？</mark>**”

* Youtube: [Noam Chomsky - Can Machines Think?](https://www.youtube.com/watch?v=Ex9GbzX6tMo)

## 1.3 AI 的两面：工程和科学

Chomsky **<mark>把 AI 分成工程的和科学的</mark>**：

* 工程的一面，如自动驾驶车等，能做出对人类有用的东西；
* 科学的一面，Chomsky 明显不认可。

他引用图灵的话：这问题 too meaningless to deserve discussion（没有讨论的意义）。

当一帮奇点理论的粉丝带着正面的期望采访 Chomsky 时，他却对人工智能这个被他深刻影响过的学科没太当回事，
他认为**<mark>气候和毁灭性武器是比奇点更紧迫的问题</mark>**。

# 2 自动定理证明兴衰记

> As a material machine economises the exertion of force, so a symbolic calculus
> economises the exertion of intelligence ... **<mark><code>the more perfect the calculus, the smaller
> the intelligence compared to the results</code></mark>**. —— W. E. Johnson
>
> 就像机器能省体力一样，符号演算能省脑力。演算越完美，付出的脑力就越少。
>
> **<mark><code>Proof is cultivated reasoning</code></mark>**.  —— Bruno Buchberger

## 2.1 自动定理证明的起源

### 数学哲学三大派

1. 逻辑主义

    * 代表人物：罗素，
    * **<mark>把数学归约到逻辑</mark>**，因此只要**<mark>把逻辑问题解决了，之上的数学问题自然就解决了</mark>**。
    * 换句话说，**<mark>把逻辑玩转了，数学就不算事儿</mark>**。

2. 形式主义

    * 代表人物：希尔伯特
    * **<mark>把数学形式化，数学过程就是把一串符号变成另一串符号</mark>**。
    * 希尔伯特设想，如果能设计一个大一统的算法，那么所有的数学问题都可以由这个算法来解答。
      这和逻辑主义精神有一定相通之处。**<mark>哥德尔后来证明这一切是不可能的</mark>**。

3. 直觉主义

**<mark>机器定理证明</mark>**的研究从某种意义上继承了罗素和希尔伯特的思想：用机器来证明和判定那些可以证明和判定的问题。
纽厄尔和司马贺的“逻辑理论家”就是早期的机器定理证明程序，他们曾经给罗素写信，期盼能得到伟人的首肯，
罗素在回信时说：“**<mark>我相信演绎逻辑里的所有事，机器都能干。</mark>**”

### 逻辑学的源头：亚里士多德三段论

自动定理证明起源于逻辑，初衷就是把逻辑演算自动化。

逻辑学的源头是**<mark>亚里士多德的三段论</mark>**：
人必有一死，苏格拉底是人，所以苏格拉底必死。

## 2.2 思想实验：Brain in a vat

把一个人脑放在可以让它继续存活的营养液里，然后插上各自传感器，再连接到电脑，
可以通过电脑准确地向这个大脑发送各自传感器刺激（例如让它觉得是在跑步的信号）。
问题：如果有这样一个人脑，那它**<mark>能否判断出自己是一个正常人体内的大脑，还是一个缸中插满传感器的孤零零的大脑</mark>**？

<p align="center"><img src="/assets/img/brief-history-of-ai-zh/Braininvat.svg.png" width="50%" height="50%"></p>

> In philosophy, the brain in a vat (BIV) is a scenario used in a variety of
> thought experiments intended to draw out certain features of human
> conceptions of knowledge, reality, truth, mind, consciousness, and meaning.
>
> Wikepedia [Brain in a vat](https://en.wikipedia.org/wiki/Brain_in_a_vat):

## 2.3 王浩（1921—1995）

可以公正地说，王浩的定理证明研究**<mark>孕育了整个理论计算机科学</mark>**。

王浩以哥德尔的权威诠释者和知音名世，但**<mark>他对哲学、逻辑学、计算机科学的原创性却被低估了</mark>**。

王浩在致获奖词时半开玩笑地说，**<mark>因为自己的个性，荣誉经常绕道而行</mark>**。

王浩的定理证明程序后来成为高级语言的基准程序，麦卡锡的 **<mark>LISP 早期就一直以王浩算法的程序作为例子</mark>**。

## 2.4 吴文俊（1919—2017）

1979 年，吴文俊的工作得到杨振宁的关注，当时的科学院大力支持吴文俊，并为他申请到两万五千美元的外汇到美国购买一台家用电脑，以实现他的吴方法。

### 高龄开始学习编程

吴文俊的长寿也体现在他的学术生命上。1979 年吴文俊 **<mark>60 岁高龄开始学习计算机编程语言</mark>**，先是 BASIC，后是 Algol，再后是 Fortran。
他在那台两万五千美元的家用电脑上不断取得新的成果。后来系统所的硬件设施改进，吴文俊相当一段时间都是上机时间最长的。

### 为人类文明做出贡献

**<mark>杨振宁曾说他最重要的成就是提高了中国人的自信</mark>**。陈省身、华罗庚、杨振宁、李政道那一批人是最早为人类文明做出贡献的中国人。
那个不长的名单里还应该有**<mark>王浩和吴文俊</mark>**。

吴文俊生平：《走自己的路》

## 2.5 哲学问题

### 有黑盒的理解不能算理解，有黑盒的证明也不能算证明

Chomsky 对统计派机器翻译的批评：**<mark>有黑盒的理解不能算理解，有黑盒的证明也不能算证明</mark>**。

### 人已经无法核实部分计算机证明的结果

* 传统的数学实践遵循**<mark>共同体过程</mark>**：**<mark>一个数学家提出证明，然后一堆同一共同体的专家来验证</mark>**，如果验证通过，定理成立。
  费马大定理的证明、庞加莱猜想的证明和**<mark>张益唐</mark>**的证明，都是这个套路。
* **<mark>有些机器证明太长，人根本看不过来</mark>**，那**<mark>怎么才算是证明了定理呢</mark>**？
    如果用一个可被信任的计算机程序验证一遍，是不是就算是证明了呢？
    罗宾斯猜想的证明就曾用 Mathematica 验证过，而 AUTOMATH 本身就是一个验证系统。
    对全自动的定理证明，验证过程更容易机械化，而计算机辅助证明可能五花八门，很难有一个统一的过程。

### 数学家的归宿

无论如何，数学共同体的实践标准在变：从**<mark>数学家之间互相核实</mark>**到**<mark>数学家信任的程序之间互相核实</mark>**。
也难怪传统的数学家在抱怨：**<mark>数学变成了有成本的实验科学</mark>**。

其实那些典型的物理科学，例如物理、化学和生物学，是以实验为本的，可重复性（reproducibility）是检验真理的标准之一。
只不过在当下，可重复性的成本太高。
当下的数学变得越来越实验，而生物学可能变得越来越后现代了。
**<mark>无论是唯心或唯理的数学，还是唯物或经验的实验科学，最终都成了共同体式的实用主义</mark>**。

吴文俊和芒福德联合得了 2006 年的邵逸夫数学奖。得奖评语最后一句，
大意是他俩都是**<mark>从纯数学的分支拓扑最后转到和计算机科学相关的研究</mark>**，
这**<mark>为数学家的未来行为模式提供了典范</mark>**。

* 吴文俊曾留学法国，法国的数学家素有关心数学史的传统。
* 吴文俊认为中国数学是巴比伦式的而不是希腊式的，**<mark>巴比伦数学讲究计算，而希腊数学讲究公理</mark>**。

### 计算模糊了理性和经验的边界

自动定理证明依靠的工具是计算机，正是**<mark>计算模糊了理性和经验的边界</mark>**。
可以登高一步说：**<mark>计算是知识演化的基础，计算也是知识民主化的工具</mark>**。

## 2.6 现状

### 时代交替 (2006)：定理证明小组被裁，深度学习论文横空出世

阿贡实验室的定理证明小组 2006 年被裁掉了，这大概算是符号派低潮的标志性事件，一个时代结束了。
这一年 **<mark>Hinton 的深度学习论文</mark>**发表在《科学》杂志上。

有些领域，**<mark>一开始就把 80% 的容易问题都解决了</mark>**，而后就一直很难，进展很慢，少有突破。
人工智能就是这样，定理证明尤其如此。**<mark>深度学习领域近来的进步更多得益于硬件</mark>**。

### 定理证明领域的名字演化

定理证明领域的名字也经历了有趣的演化。

* 最早都叫机器定理证明（Mechanical Theorem Proving），
* 后来改叫自动定理证明（Automatic Theorem Proving），
* 再后来叫自动演绎（Automated Deduction），目前都叫自动推理（**<mark><code>Automated Reasoning</code></mark>**）。

原因很简单，演绎（deduction）只是推理的一种，现在归纳（induction）、溯因（abduction）也都算成推理了。

贝叶斯推理，可以叫 Bayesian Logic，或 **<mark><code>Bayesian Inference</code></mark>**，也可以叫 **<mark><code>Bayesian Reasoning</code></mark>**。

## 2.7 结束语

### 数学家不把逻辑学家当回事

王浩曾经**<mark>抱怨数学家不把逻辑学家当回事</mark>**。图灵也有类似的说法：逻辑学家给数学家提供了有营养的饭菜，但做的不够美味，数学家不爱吃。

### 逻辑似乎处于一切科学的底部，因为逻辑探索一切事物的本质

维特根斯坦曾有言：**<mark>“逻辑似乎处于一切科学的底部 —— 因为逻辑的研究探索一切事物的本质。”</mark>**
但数学家不觉得他们非得趴在逻辑学家的背上。自动定理证明的状况与此相关，数学家没觉得这玩意儿有用，人工智能的两派人马都不待见。

哈尔莫斯（Paul Halmos）是数学家，但也曾涉猎逻辑，在自传里拿逻辑开玩笑，
说**<mark>即使有人证明了黎曼猜想是不可判定的（哥德尔就是这么猜测的），数学家睡一觉，第二天起来还是该干嘛干嘛</mark>**。

### 两个 Alpha-zero 下棋，我们人类已经看不懂了

法国数学家 David Ruelle，《Post-Human Mathematics》：
**<mark>也许某一天，我们人类看机器做数学，就像黑猩猩看我们阅读伽罗瓦理论</mark>**。
其实这种情况已经发生了：**<mark>两个 Alpha-zero 下棋，我们人类已经看不懂了</mark>**。

# 3 从专家系统到知识图谱

> **<mark><code>The test of all knowledge is experiment</code></mark>**. —— Feynman Lectures on Physics（《费曼物理学讲义》）

## 3.1 机器归纳法：用现在的话说就是机器学习

## 3.2 知识表示

知识表示一直是人工智能不温不火的一个领域，催生者是专家系统和自然语言理解。

### 逻辑是最方便的知识表示语言

**<mark>逻辑是最方便的知识表示语言</mark>**，从亚里士多德开始人们就熟悉，逻辑同时具有各种数学性质。
任何一本逻辑入门书都会有那个著名的苏格拉底的例子：人必有一死，苏格拉底是人，所以苏格拉底必死。

### 心理学与语言学

知识表示的另一个来源是心理学和语言学，例如概念的上下位继承关系最方便的表示方式是树而不是一阶逻辑。

心理学实验表明，**<mark>人在回答“金丝雀会飞吗？”要比回答“鸟会飞吗？”花的时间长</mark>**，
要回答第一个问题，人要再**<mark>做一次“金丝雀是鸟”的推理</mark>**。
因为人在存储知识时只存储抽象的，这是空间经济的考虑。

心理学家米勒和 Chomsky 等一起开拓了认知科学，他最出名的论文大概就是那篇“魔力数字七”（The Magic Number Seven）。

### Minsky 的框架：面向对象

框架（Frame）就是类型。

* 金丝雀是鸟，所有鸟的性质自动流传给金丝雀，鸟能飞，金丝雀也能飞。
* 苹果手机是手机，手机能打电话，苹果手机也能打电话。

框架导致了**<mark>面向对象（OO，Object-Oriented）的设计哲学</mark>**，相关的程序设计语言都受此影响。

### 当一个概念有了成熟的实现时，就自动脱离了人工智能

从这个意义上还真验证了：**<mark>当一个概念有了成熟的实现时，就自动脱离了人工智能</mark>**。

## 3.3 知识库

### 把人类的常识编码，建成知识库

想法：**<mark>把人类的常识编码，建成知识库</mark>**。这个新项目叫 Cyc，这其实就是**<mark>最早的知识图谱</mark>**。

* 雷纳特坚定地支持他老师费根鲍姆的知识原则（Knowledge Principle）：**<mark>一个系统之所以能展示高级的智能理解和行为，主要是因为在所从事的领域所表现出来的特定知识</mark>**：概念、事实、表示、方法、比喻以及启发。
* 雷纳特甚至说：“**<mark>智能就是一千万条规则</mark>**。”

“知识汤”（knowledge soup）的说法：我们脑子里的知识不是一坨知识，而是好几坨知识，每一坨内部是一致的，但坨和坨之间可能不一致，坨和坨之间是松散耦合的。

Cyc 的原始目标更像是当今的维基百科，不过维基百科的受众是人，Cyc 的用户是机器。

### 学习只在已知事物的边缘发生

雷纳特曾说：“学习只在已知事物的边缘发生，**<mark>所以人们只可能学到与自己已知相似的新东西</mark>**。
如果你试图学习的东西与你已知的东西距离不远，那么你就能学会。这个边缘的范围越大（你已知的东西越多），就越有可能发现新的东西。”

## 3.4 语义网（HTTP/HTML）

由专家系统一脉相传的这一派自身的逻辑功力不够，另一方面，他们的工程实践又略显欠缺。
直到歪打正着的万维网支持者之一 Tim Berners-Lee 提出“语义网”（Semantic Web），他们认为机会来了。

伯纳斯-李因为草根且便捷的 HTTP 协议和 HTML 出了名，被各种媒体称为万维网的发明人。
20 年后，伯纳斯-李不负所望得了 **<mark>2016 年图灵奖，这大概是图灵奖有史以来含金量最低的一个</mark>**。

## 3.5 计算机科学的划分

<p align="center"><img src="/assets/img/brief-history-of-ai-zh/computer-science-categories.png" width="60%" height="60%"></p>
<p align="center">计算机科学的划分</p>

## 3.6 对知识做梳理是人类最早的智力活动之一

对人类的知识做梳理是**<mark>人类最早的智力活动之一，也是人类的集体自我意识</mark>**。

当欧洲还在黑暗时期时，伊斯兰科学迎来了黄金期。
法拉比（Al-Farabi）是伊斯兰世界第一个自成系统的哲学家，他对亚里士多德的注释和对柏拉图与亚里士多德哲学的调和对后代阿拉伯哲学和西方哲学影响很大，
被称为“亚圣”（Second Master 或者 Second Teacher），首圣当然是亚里士多德了。

# 4 第五代计算机的教训

> **<mark><code>People learn from history that people never learn from history</code></mark>**. -- Georg Wilhelm Friedrich Hegel（黑格尔）
>
> **<mark><code>Those that fail to learn from history, are doomed to repeat it</code></mark>**. Winston Churchill（丘吉尔）

日本早年神经网络研究的先驱福岛邦彦和甘利均一。

当下流程的**<mark>卷积神经网络 CNN 的源头就是福岛邦彦的工作</mark>**。

在福岛邦彦和甘利均一的壮年，日本都把资金投入到了五代机，他们没赶上好时候。

# 5 神经网络简史

> I bet the human brain is a kludge. Marvin Minsky

自图灵提出“计算机与智能”起，就一直有两派观点：

* 一派认为实现人工智能**<mark>必须用逻辑和符号系统</mark>**，这一派看问题是自顶向下的；
* 还有一派认为通过**<mark>仿造大脑可以达到人工智能</mark>**，这一派是自底向上的，他们认为如果能造一台机器，**<mark>模拟大脑中的神经网络</mark>**，这台机器就有智能了。

## 5.1 神经网络的初创文章，1943

神经网络的原创文章发表于 1943 年，两位作者都是传奇人物：麦卡洛克（Warren McCulloch）和皮茨（Walter Pitts）。Pitts 打小就喜欢数学和哲学，初中时就读过罗素的《数学原理》，还和罗素通过信。

### A Logical Calculus of the Ideas Immanent in Nervous Activity, 1943

神经网络的开山之作：**<mark><code>A Logical Calculus of the Ideas Immanent in Nervous Activity</code></mark>**，发表在 Bulletin of Mathematical Biology 上。

* 这篇文章成了**<mark>控制论的思想源泉之一</mark>**。
* 这篇文章只列了三篇貌似不相关的参考文献，卡尔纳普的《语言的逻辑句法》，希尔伯特和他学生阿克曼合著的《数理逻辑基础》，怀特海和罗素的《数学原理》。

## 5.2 维纳

**<mark>控制论的创始人维纳</mark>**（Norbert Wiener）早年自称神童，他爸是哈佛大学教授，曾经带着他到英国见过罗素，但罗素特不喜欢这孩子和他爹。
自打进入 20 世纪后，甭管哪门哪派的学问，最后都能扯到罗素那儿。

维纳后来也在哈佛大学任教，但不被主流数学家喜欢，没拿到终身教职。最后到了隔壁的麻省理工学院落脚，
在“二战”时搞了点武器研究。那时最好的数学家和物理学家都参与了造原子弹的“曼哈顿”计划，维纳却没沾边。
这也许同他的个性有关系，他的同事和家人都觉得他对数学之外的事情反应迟钝。**<mark>维纳提出“控制论”后出了大名</mark>**。

维纳曾写过两卷本的自传：**<mark>《昔日神童》（Ex-prodigy）和《我是数学家》</mark>**。
不喜欢维纳的人开玩笑说，应该是《昔日数学家》和《我是神童》，嘲讽维纳的数学不入主流，同时暗示维纳对自己神童身份的过高自视。

维纳无论如何首先是一位**<mark>严谨的数学家</mark>**，
而 McCulloch 则被人称为是**<mark>浪漫的科学家</mark>**。所谓“浪漫”不是指生活，而是说他对科学思想的表述方式。

维纳曾经把**<mark>为大脑建模作为他学术生涯的最后野心</mark>**。

### 强化学习之路：维纳 -> 阿比卜 -> Andy Barto -> Richard Sutton

阿比卜的“杂学”体现在他那本科普书《大脑、机器和数学》里，其实他本科毕业论文已初露端倪，题为“Turing Machines, Finite Automata, and Neural Nets”。

阿比卜后来创办了麻省大学的计算机系，并延揽一帮人工智能人马，其中有后来以强化学习出名的巴托（Andy Barto），使麻省大学的人工智能曾在很长一段时间都处于领先地位。

## 5.3 罗森布拉特和感知机

神经网络研究的后一个大突破是在 1957 年。康奈尔大学的实验心理学家 Frank Rosenblatt 在一台 IBM-704 计算机上模拟实现了一种他发明的叫作**<mark>“感知机”（Perceptron）的神经网络模型</mark>**。
这个模型可以完成一些简单的视觉处理任务。这在当时引起了轰动。

### Perceptrons: An Introduction to Computational Geometry

影响巨大、“是也非也”的书：《感知机：计算几何学》（Perceptrons: An Introduction to Computational Geometry）。

* 在书中，Minsky 和佩珀特证明**<mark>单层神经网络不能解决 XOR（异或）问题</mark>**。
* 异或是一个基本逻辑问题，**<mark>如果连这个问题都解决不了，那神经网络的计算能力实在有限</mark>**。

感知机的失败导致了神经网络研究的式微，用加州理工学院的集成电路大佬米德（Carver Mead）的话说是“二十年大饥荒”。
Minsky 1988 年在《感知机：计算几何学》一书再版时，删除了第一版中对罗森布拉特个人攻击的句子，并手写了 In memory of Frank Rosenblatt。

## 5.4 神经网络的复兴

### 解决 XOR 问题：神经网络多加一层+后向传播

1974 年，哈佛大学的一篇博士论文证明了**<mark>在神经网络多加一层，并且利用“后向传播”（back-propagation）学习方法，可以解决 XOR 问题</mark>**。

* Paul Werbos 这篇文章刚发表时并没引起多少重视，那时正是神经网络研究的低谷，文章不合时宜。
* Paul Werbos **<mark>也是递归神经网络 RNN 的原创者</mark>**。但在深度学习大火后，他的兴趣转向了量子力学。

### Hopfield 神经网络：来自物理学而非生物学的突破

神经网络在 20 世纪 80 年代的复兴归功于物理学家 John Hopfield。

* 1982 年，Hopfield 提出了一种新的神经网络，可以**<mark>解决一大类模式识别问题</mark>**，还可以给出一类组合优化问题的近似解。
  这种神经网络模型后来被称为 Hopfield 网络。
* 1984 年，Hopfield**<mark>用模拟集成电路实现了自己提出的模型</mark>**。

Hopfield 模型的提出振奋了神经网络领域。

* 神经网络的**<mark>这次复兴和生物学没啥关系</mark>**，它既不是来自生物学的刺激，也没有给生物学送去任何慰藉。
* 倒是它**<mark>来源于物理学家，并引起了物理学家的关注</mark>**，曾经一批对复杂系统感兴趣的物理学家在交叉学科杂志上接二连三地发表文章。

### 连接主义运动（Hinton）

一帮早期神经网络研究的“幸存者”，在生物学家克里克（Francis Crick）
和认知科学大佬诺曼（Don Norman）的鼓励下，开始了**<mark>连接主义（Connectionism）运动</mark>**。
领导者：

* 两位心理学家鲁梅尔哈特（David Rumelhart）和麦克利兰德（James McLelland），
* 一位计算机科学家辛顿（**<mark><code>Geoffrey Hinton</code></mark>**）。

连接主义运动的成果之一就是那本被称为 PDP（**<mark><code>Parallel Distributed Processing</code></mark>**）的著名文集（分两卷）。
此书的出版给认知科学和计算机科学吹了股春风，**<mark><code>被神经网络新秀称为“圣经”</code></mark>**。

#### Rumelhart -> Michael Jordan -> Andrew Ng

连接主义运动也培养了一堆新人，并使得加州大学圣地亚哥分校的认知科学系成为同类系科的佼佼者。

* Rumelhart 后转往斯坦福大学任教，乔丹（Michael Jordan）就是他的学生，而吴恩达（Andrew Ng）又是乔丹的学生。
* Rumelhart 的另一名学生格 Robert Glushko 后来远离本行，跟随硅谷互联网早期英雄 Marty Tennenbaum 创立了一家公司，赚了一票钱。
  格鲁什科捐钱设立了“Rumelhart 奖”来奖励神经网络的研究者，**<mark>辛顿成了第一位获奖者</mark>**。

#### Chomsky：统计的方法不优雅，只是模仿而不是理解

Chomsky 认为统计的方法不“优雅”（elegant），只是模仿而不是理解。
**<mark>会骑自行车不算理解，对自行车为什么不倒，能说清道理，才算理解</mark>**。

#### Peter Norvig：在理解之前不妨碍模仿先上

谷歌的研发总监 Peter Norvig 为统计方法辩护时说：
简单的模型（如 Chomsky 理论，以及后来的各种改进版本）不能解决复杂的问题，**<mark>人工智能的进一步发展必须两条腿走路</mark>**。

诺维格在加入谷歌之前曾是加州大学伯克利分校的计算机教授，他对两派都了如指掌，在学术界和工业界都被尊重，他写的《人工智能》是最流行的教科书。

## 5.5 深度学习

**<mark>神经网络在 20 世纪 80 年代的光芒被后来的互联网掩盖了</mark>**。

* 但这几年，恰恰是互联网产生的海量数据给了神经网络更大的机会。
* 人工智能学者在计算机系曾经是最抬不起头的，这几年却人人都变成了大知识分子。

### 网络对应的概念：一层网络就是一个函数

神经网络由一层一层的神经元构成。层数越多，就越深，所谓深度学习就是用很多层神经元构成的神经网络实现机器学习的功能。
理论上说，

* **<mark>如果一层网络是一个函数的话，多层网络就是多个函数的嵌套</mark>**。
* **<mark>网络越深，表达能力越强，但伴随而来的训练复杂性也急剧加大</mark>**。

### Hinton 2006：降维和逐层训练，使深度学习的实用化成为可能

辛顿是**<mark>深度学习的先驱</mark>**，他和学生在 **<mark><code>2006</code></mark>** 年发表的两篇文章开辟了这个新领域，

* 登在《科学》上的那篇**<mark>提出了降维和逐层预训练的方法</mark>**，使得**<mark>深度学习的实用化成为可能</mark>**。
* 深度神经网络**<mark>最后几层的每个节点都可对应于某些概念</mark>**。
  这是神经网络的一大进步，调和了与符号派的矛盾。至于符号派买不买账，就是另一回事了。

# 6 计算机下棋简史

> Play is the beginning of knowledge.—— George Dorsey

## 6.1 图灵， ~1944

* 二战没结束时，图灵就研究计算机下棋，他 1947 年编了第一个下棋程序。
* Donald Michie 是图灵的追随者，1950 年试着在纸上模拟程序，和图灵对弈。
* Dietrich Prinz 接着图灵的思路，在 1951 年写了一个残局程序，能在离将死还有两步的情况下，找到最优解。这个问题也被称为**<mark>“两步将死”（mate-in-two）问题</mark>**。

## 6.2 冯诺依曼，《博弈论》提出 MiniMax 算法， 1944

### 《博弈论》, 1944

**<mark>几乎和图灵同时，冯诺伊曼也在研究计算机下棋</mark>**，
他和经济学家摩根斯顿合作的《博弈论》1944 年出版，其中首先**<mark>提出两人对弈的 Minimax 算法</mark>**。

Minimax 算法中，二人对弈的一方为 max，另一方为 min，max 一方的评估函数要越高越好，min 一方的则越低越好。

* max 和 min 的对弈就形成了**<mark>博弈树</mark>**。
* 树的增长是指数式的，当树很深时，树的规模会变得不可控。
* 麦卡锡首先提出α-β剪枝术以控制树的增长。

## 6.3 香农：开创计算机下棋的理论研究，1950

### Programming a Computer for Playing Chess, 1950

香农（Claude Shannon）1950 年在《哲学杂志》发表“计算机下棋程序”（Programming a Computer for Playing Chess）一文，
开启了计算机下棋的理论研究，其中主要思路在“深蓝”和 AlphaGo 中还能看到。

* 香农**<mark>把棋盘定义为二维数组</mark>**，
* 每个棋子都有一个对应的子程序计算棋子所有可能的走法，
* 最后有个评估函数（evaluation function）。

传统的棋局都把下棋过程分为三个阶段：开局、中局和残局，不同阶段需要不同的技术手段。

香农的论文**<mark>引用了冯诺伊曼的《博弈论》和维纳的《控制论》</mark>**。

## 6.4 IBM 深蓝战胜卡斯帕罗夫， 1997

1997 年 5 月 11 日，老卡认输，“深蓝”成了第一位战胜当时世界冠军的机器。
事后，卡斯帕罗夫回忆：第二局是关键，机器表现超出他的想象，**<mark>它经常放弃短期利益，“showing a very human sense of danger”</mark>**。

在“深蓝”赢了卡斯帕罗夫之后，职业棋手并没有因此而改行，他们反而更多地依赖计算机来训练。
**<mark>机器作为教练，反而更快地帮助人类棋手进步</mark>**，因为过去的孩子从来就没有机会能和特级高手比赛。

## 6.5 AlphaGo：首次引入了强化学习

谷歌的 AlphaGo 首次引用了强化学习（Reinforcement Learning），让**<mark>机器和自己对弈学习</mark>**。
强化学习的发明者是巴托（Andy Barto）和他的学生萨顿（Richard Sutton）。

**<mark>强化学习 80 年代就发明了，但一直不被重视，是 AlphaGo 使得它焕发新生</mark>**。

# 7 自然语言处理

> **<mark><code>the noblest pleasure is the joy of understanding</code></mark>** - Leonardo da Vinci
> 
> It is not our aim to refine or complete the system of rules for the use of our words in unheard-of ways. - Wittgenstein

## 7.1 Chomsky

### 《句法结构》

**<mark>Chomsky 之于语言学和认知科学，就像图灵之于计算机科学</mark>**。
他认为，

* 所有的语言（人工或自然）都有类似的句法结构，
* 语言的结构是内在的，而不是通过经验习得的，
* 代表作《句法结构》。一本小册子，不需要什么背景就能读。

  Brown (1988，1990)是统计派的奠基作品，正文只有 6 页，虽是学术论文，却非常可读。

### 经验主义靠近科学，理性主义靠近数学

从某种意义上说，**<mark>行为主义是极端的经验主义</mark>**。

* **<mark>所有黑盒理论，无论是神经网络还是统计派，在 Chomsky 眼里都属行为主义</mark>**。
* **<mark>Chomsky 认为理论应该先于事实</mark>**。他常以遗传学祖师爷孟德尔为例，但孟德尔常常删改不支持理论的数据。

Chomsky 认为心身（mind-body）问题是个伪问题，难度倒不在于如何定义 mind，而在于连什么是 body 这样貌似简单的问题都无法明确地说清。

* 他认为 **<mark>mind 的研究终究会变成像物理学、化学那样的学问</mark>**，只不过现在还要**<mark>用心理学的术语逐步获得进展</mark>**。
* **<mark>语言学是突破口之一，由此可以找到 “mind” 的物理机制</mark>**。
* 从这个意义上说，Chomsky 也不完全反对经验主义。

### 语言学的牛顿？

Chomsky 比较了笛卡儿和牛顿的理论，认为牛顿为物质世界提供了一个解释理论，但笛卡儿却没有为语言的创造性使用提供满意的解释。
他自认为他正在向这个方向前进。
也有人称 **<mark>Chomsky 是语言学的牛顿</mark>**。

科学方法素有 **<mark><code>explanation</code></mark>** 和 **<mark><code>redescription</code></mark>** 之分。

* 统计方法可看作一种 redescription，但不是 explanation。
* Chomsky **<mark>不认可语言学的统计方法</mark>**。

### 活着的人里被引用次数最多的知识分子？

Chomsky 是活着的人里被引用次数最多的知识分子，即使从苏格拉底算起，他的引用数也可排进前十。

* 他的时事评论几十年来都被广为关注，这一点颇像他的偶像罗素。
  他的独特政治观点体现在他对当代政治事件的评论上。
* 人们轻率地把 Chomsky 划为左派，其实，他是反建制者，永远怀疑权威，永远同情人民。
* Chomsky 作为犹太人，却不被以色列接受，因为他同情巴勒斯坦的立场。以色列甚至拒绝给 Chomsky 发签证。
* Chomsky 在任何地方的学术演讲，**<mark>最后总要“饶”一段儿同等时间的政治评论</mark>**，就像演出的返场。

Chomsky 敬仰的人不多，无政府主义者**<mark>乔治·奥威尔</mark>**是一个，**<mark>罗素</mark>**是另一个。
很多人拿 Chomsky 和罗素做比较，

* 罗素在出版了《数学原理》后很少再有原创的知识贡献，兴趣转向政治；
* Chomsky 在《句法结构》之后也成为一位社会活动家和公共知识分子。

但 Chomsky 仍然不断有科学成果出来。罗素被下过两次大牢，Chomsky 1967 年因为反越战被捕，和诺曼·梅勒关在一起。

## 7.2 统计派又来了

### 我每开除一名语言学家，语音识别系统的性能就提高一点

Frederick Jelinek 是这个小组的核心。贾里尼克的学术训练是信息论，统计是他们这一派人最自然的工具。
他的金句是：“**<mark>我每开除一名语言学家，我的语音识别系统的性能就提高一点</mark>**。”

IBM 小组的成员之一柯克（John  Cocke）因为 RISC 架构在 1987 年就得了图灵奖。
他在图灵奖的致辞中说，**<mark>计算机性能的提升主要源于三个方面：算法、编译器和体系结构</mark>**。
这三个方面是按重要性大小排序的，但他的名声却主要来自于他认为重要性最小的体系结构。

其实最早提出机器翻译的 Warren Weaver 的思路就是统计。但 Chomsky 登场后，统计方法基本就没饭吃了。

* Chomsky 的理由很简单，语言的可能性是无限的，统计不可能解决问题。
  Chomsky 对统计方法的排斥，恰似波普尔对卡尔纳普归纳法的批判。
* Chomsky 不喜欢统计派的一个理由是他们太像行为主义了：在翻译的统计方法中，平行语料的左边就是刺激，右边就是反射。

### 工程师根本不需要语言学知识，也不需要懂源语言或目标语言

2004 年，Franz Josef Och 加入谷歌。谷歌海量的数据让欧赫如鱼得水。谷歌翻译器迅速成为行业标杆。
2014 年欧赫在谷歌呆了十年后先后加入两家基因测序公司。

统计方法的另一个好处是**<mark>工程师根本不需要语言学知识，也不需要懂源语言或目标语言，就可从事机器翻译</mark>**。
谷歌翻译团队就没什么科班出身的语言学家。欧赫认为语言学知识对翻译没什么用处，有时还会起反作用。

## 7.3 神经翻译是终极手段吗？

### Google Neural Machine Translation (GNMT), RNN-based, 2016

2016 年，谷歌发布神经机器翻译（GNMT）系统，再次大幅提高机器翻译的水平。

* 和谷歌更早期的 **<mark><code>Phrase-Based</code></mark>** Machine Translation (PBMT) 不同，神经翻译的**<mark>基本单位是句子</mark>**，
* 谷歌使用了**<mark>循环神经网络</mark>** RNN 做 Sequence to Sequence 的学习，
* 硬件设备是谷歌自己的 TensorFlow 平台。

神经翻译相比谷歌早期的基于短语的翻译系统，误差降低了 60%，这是翻译质量巨大的提升。
这项工作已经开源。

### Facebook, speed 10x, CNN-based, 2017

2017 年，Facebook 进一步提高了翻译效率。他们用自己擅长的卷积神经网络 CNN，进行序列到序列的学习。
Facebook 号称，英文-德文和英文-法文翻译的基准测试表明，

* 他们的结果在准确度上不输谷歌，
* 而在计算**<mark>速度上则比谷歌的 RNN 有一个数量级的提升</mark>**。

RNN 和 CNN 两种神经网络架构，分别被谷歌和 Facebook 支持。性能的此消彼长也被视为两家公司的竞争。
真难预料神经网络还有多大的潜力可以挖掘。

### 翻译只是数据问题，不是语义问题？

Chomsky 们也许会接着质疑，这种翻译算理解吗？

**<mark>也许翻译根本就不是理解的问题，翻译本身并不需要解释，翻译只是翻译而已，翻译只是数据问题，而不是语义问题</mark>**。

没有 Chomsky，我们还要在黑暗中摸索，但有了 Chomsky，是不是又曾经束缚了我们探索其他方法的可能性。

## 7.4 IBM wason：知识库/知识图谱+浅层推理

现在的问答系统依靠常识和知识，同时也依靠浅层的推理。知识图谱是核心。

在 Jeopardy！节目中出现过的问题，95% 都能在维基百科中找到答案。

* 沃森参赛的版本的**<mark>知识库只有 4TB</mark>**，其中包含了所有维基百科的正文，真的不大。
* 除了半结构化的知识图谱，沃森**<mark>还使用了开源搜索引擎</mark>**。

    把搜索的结果文档的标题与维基百科词条进行匹配，如果在维基百科中能找到，就把搜索结果列入候选答案。
    再把候选答案反馈给搜索引擎，进一步对返回结果做证据支持的处理，然后给出答案。

* 硬件系统是一个有 **<mark>90 台 IBM Power 750 的集群</mark>**，每台配一个 IBM Power 78 核处理器，每核 4 线程，所有一共 720 核，2880 线程；内存 16TB，**<mark>所有的知识图谱都放在内存里了</mark>**。

    按照 Linpack 基准程序，这台计算机的算力相当于当年排名第 500 的超级计算机的一半，成本只有 300 万美元。
    同沃森带来的巨大广告效应相比，这真不算什么。

IBM 吸取了深蓝的教训，沃森在 Jeopardy！节目上取得的宣传成功后，很快变成了 IBM 人工智能事业的品牌，IBM 很快推出了沃森金融、沃森医疗、沃森教育等。
现在 IBM 整个公司都围绕沃森转型了，也许 IBM 觉得“人工智能”这个词儿太俗了，他们非要标新立异地自诩为“认知计算”。

## 7.5 总结

### 一个人工智能问题一旦解决，就不再是人工智能问题

就像**<mark>一个哲学问题找到了科学的角度（formulation），就不再是哲学问题</mark>**一样，
一个人工智能问题一旦解决，就不再是人工智能问题。

* 大概很快人们就会认为语音问题不再是人工智能的核心问题。
* 如果说语音翻译不涉及自然语言理解和语义，可能也不会有什么异议。

2011 年 5 月，麻省理工学院为配合 150 周年校庆，召开了“大脑，心，机器”的研讨会（Brain, Mind and Machine Symposium）。

* Chomsky 批评当下流行的神经网络和统计方法，Chomsky 认为神经网络是黑盒子，并没有给我们提供解释，故而没有提供知识。
* 时任谷歌研发总监的诺维格（Peter Norvig）很快回应 Chomsky，他批评语言学的规则在自然语言处理上，根本就没用。

### 可解释性

有人开始用“两种文化”来总结 Chomsky 和诺维格的隔空掐架。

* Chomsky 对人工智能的批评的核心在于“可解释性”。AlphaGo 不能解释自己下棋的路数，算不算会下棋呢？
* 也可以反过来说，只有解释了，人类才能从中得到洞见，学习知识。但解释是不是也有层次，只有学会牛顿力学，才能学会相对论和量子力学？就如维特根斯坦所说的梯子的比喻，爬上房顶，梯子才能扔掉，梯子就是解释。其实，即使**<mark>人类在不理解力学的时候，就会造弹弓了</mark>**。对那时的人类，弹弓的工作原理就是黑匣子。

### 不求甚解的工程师 vs. 追求终极知识的科学家

Chomsky 和诺维格分别所代表的两种人关心的是两种不同的问题。

* 一种人力图**<mark>打造实用的工具</mark>**，没有解释也能凑合，他们是不求甚解的工程师；
* 另一种人**<mark>寻求终极的知识</mark>**，他们是科学家。

只不过，在计算机科学这个特定的学科中，科学家和工程师的角色变换太快，这门学科的开拓者，很多都是身兼二职，例如图灵和冯诺伊曼

# 8 向自然学习：从遗传算法到强化学习

> **<mark><code>Natural selection is a mechanism for generating an exceedingly high degree of improbability</code></mark>**. —— Ronald Fisher
>
> 自然选择就是能生成极不可能之事的机制。

## 8.1 从生物学里找计算的模型：两条传承脉络

**<mark>从生物学里找计算的模型</mark>**，一直是人工智能的研究方向之一，学术上大致有两条传承的脉络：

1. McCulloch 和 Pitts 的**<mark>神经网络</mark>**，演化到今天成了**<mark>深度学习</mark>**；
2. 冯诺伊曼的**<mark>细胞自动机</mark>**，历经遗传算法、遗传编程，其中一条支线最后演变成了今天的**<mark>强化学习</mark>**。

## 8.2 John  Holland 和遗传算法

Holland 在晚年接受采访时如此评论麦卡锡和 Minsky：

* 美国西部的人工智能由麦卡锡代表，他们干净（neat），一切讲究逻辑；
* 东部的领袖自然是 Minsky，他们邋遢（scruffy），做事比较随意（adhoc）。

但他们的**<mark>共性是都对机器学习不太感兴趣</mark>**。

### Ronald Fisher, 英国统计学家费舍

Holland 说他**<mark>自己的思想被学界逐渐接受，是在他的学生都出了名之后</mark>**。

* 对 Holland 影响最大的一本书是英国统计学家费舍（Ronald  Fisher）的《自然选择的遗传理论》（The Genetical Theory of Natural Selection）。
* 无神论者道金斯（Richard  Dawkins）称费舍是**<mark>达尔文之后最伟大的生物学家</mark>**。

### 进化和遗传是族群学习的过程，机器学习可以此为模型

费舍**<mark>把孟德尔的遗传理论和达尔文的自然选择结合起来</mark>**。
Holland 由此得到启发：**<mark>进化和遗传是族群学习的过程，机器学习可以此为模型</mark>**。

### 遗传算法

遗传算法就是**<mark>模拟种群（population）的进化过程</mark>**。其结构可以用下列伪代码大致表示。

1. 随机生成初始群体。
2. 主循环（停机的标准可以是迭代次数，或者适应度达到某个要求）。
    * 2.1 执行策略，计算当前群体中所有个体的适应度；
    * 2.2 从当前群体中，选择精英作为下一代的父母；
    * 2.3 将选出的精英父母配对；
    * 2.4 以极小概率将子代变异；
    * 2.5 将子代个体添加到新群体中。

从程序中，我们马上可以理解进化中**<mark>“优胜劣汰”的算法含义</mark>**。

## 8.3 遗传编程

在遗传算法中，种群是数据，更进一步的想法是：如果种群变成程序的话，进化是不是仍然可行呢？
Holland 的学生寇扎（John Koza）在 1987 年给出了一个思路，并把它命名为“遗传编程”（Genetic Programming）。

物理学家多依奇（David Deutsch）用生物进化来类比知识的进化，他是哲学家波普尔（Karl Popper）的粉丝，并常常套用波普尔的科学哲学术语。
他说猜想就像变异，批评和实验就像选择，而交叉学科就是配对了。
从这个意义上说，知识的增长更像是遗传编程。

遗传编程的结构和遗传算法差不多，

* 一组程序就一个特定的问题给出解答，按照执行结果的好坏给所有程序排序。
* 程序本身也是数据，自然也可以修改。
* 在遗传编程里，变异就是对程序做微小调整。
* 交叉和配对就是将两个表现优异的程序互相嫁接。

寇扎后来还引入了“基因重复”（duplication）和“基因删除”（deletion）等生物学概念，以提升遗传编程的效率。

遗传算法本身就需要大量的数据，遗传编程需要的数据量自然更大，这对计算能力提出了新的需求。

遗传算法的稳定性一直就是研究课题，遗传编程的数学性质自然更加复杂。

## 8.4 强化学习

“人工智能”这个词儿的流行是在 20 世纪 70 年代中期，按照阿比卜的一家之言：**<mark>人工智能是控制论的替代品</mark>**，至少从时间轴上看，这不算错。


### 一个刚出生的孩子，怎么学会对环境的适应

巴托和萨顿关心**<mark>更原始但也更抽象的可适应性</mark>**。
一个刚出生的孩子，怎么学会对环境的适应。

* 在**<mark>监督式学习中，目标是清楚的</mark>**。
* 但婴儿不知道目标是什么，不知道自己要什么。**<mark>通过与外部世界的不断交互，婴儿受到奖励或惩罚，由此强化对外部世界的认知</mark>**。

### 数学基础：马尔科夫决策过程和动态规划

强化学习的理论基础之一是**<mark>马尔科夫决策过程</mark>**。

* 强化学习的**<mark>主体是 Agent</mark>**，Agent 和环境互动。
* 强化学习就是 Agent 根据经验改变策略以期达到长期最大奖赏的过程。

强化学习的另一个理论基础是动态规划。

* 贝尔曼（Bellman）在 20 世纪 50 年代就发明了动态规划。
* 萨顿和巴托也承认在强化学习早期，受到动态规划的启发。巴托一度在他的强化学习讨论班上让研究生分工研读贝尔曼的经典著作《动态规划》（Bellman 1957）

### 在计算能力的约束下，强化学习的环境不宜太复杂

* 萌芽期的强化学习的例子都是游戏，如贝尔曼的“老虎机  ”和塞缪尔（Samuel）的跳棋。
* 游戏的环境相对容易定义，在棋类比赛中，环境就是对手和规则。
* 强化学习被用来下围棋不是偶然的。

**<mark>如果整个世界是完全随机的，那么强化学习就要失效</mark>**，学还是不学对结果没有什么影响。

巴托和萨顿有时也把强化学习称为“享乐主义”（hedonistic），也即学习系统想**<mark>最大化环境对自己的某种反馈</mark>**。

### exploration vs. exploitation

强化学习中有所谓“抬头看路”（探索，exploration）和“低头拉车”（苦干，exploitation①）之分。
探索就是看看有没有别的选择，苦干就是专注于当前的选择。

### learning rate

在强化学习中，用希腊字母 ε 表示学习率（**<mark><code>learning rate</code></mark>**），
**<mark>值越小，能用于探索的时间就越少</mark>**，绝大部分时间是在苦干。

### 减少状态空间搜索

遗传算法和强化学习有一个共同点：**<mark>效果要等到多步以后才能看到，这是和监督式学习的主要不同</mark>**。
这就需要尽可能多地访问所有的状态，这样效率就会受到影响。

* **<mark>蒙特卡洛模拟</mark>**是一种减少状态空间搜索的有效办法。
* 最近也有利用深度学习来压缩需要表示的状态空间数目。
  这还有点意思，本来强化学习初衷是探索生物体学习的模型，现在神经网络又成了强化学习的工具。

当状态空间很大时，强化学习可以和蒙特卡洛方法或深度神经网络结合，就使用了蒙特卡洛方法

### AlphaGo 让强化学习一夜之间成为显学

强化学习作为机器学习的一个分支，一直没得到重视。谷歌的 AlphaGo 赢了李世石之后，
**<mark>强化学习作为 AlphaGo 的核心算法，一夜之间成为显学</mark>**。这当然要归功于萨顿和巴托多年的坚持。

巴托的“可适应系统”实验室，在神经网络不景气时，曾经收留过一批无家可归的学术浪人，其中就有吴恩达的老师乔丹。
事实上，吴恩达的成名作就是**<mark>用强化学习来控制无人直升机</mark>**。

### 萨顿：开创强化学习，留有一点控制论的影子

萨顿 1979 年到麻省大学跟随巴托和阿比卜，由此**<mark>开创强化学习</mark>**。

* 他一直认为强化学习是理解智能的关键。
* 在整个人工智能的各个分支里，大概**<mark>只有强化学习还留有点儿控制论的影子</mark>**。

一旦一个算法被天才发明，并成功地在一个领域里得到应用，自然会有二流人才前赴后继把这个算法在其他领域发扬光大。20 世纪 80 年代的神经网络如此，当下的强化学习也如此。

早年有人质疑遗传算法算不算机器学习，他们认为遗传算法是一种近似优化算法，不能算机器学习。但从某种意义上，任何机器学习算法都是一种优化算法。

### 强化学习 vs. 监督式学习：第一人称叙事 vs. 第三人称叙事

如果从写作的角度看，

* 强化学习更像是第一人称叙述，**<mark>Agent 就是“我”</mark>**，外部世界（包括他人）都是“环境”  。
* 监督式学习更像是第三人称叙述，作者在**<mark>用一只上帝的眼睛洞察世界，对错分明</mark>**。

第一人称的学习要比第三人称的学习**<mark>更本质</mark>**。

Stuart Russell 和 Peter Norvig 在《人工智能：一种现代方法》里说
“**<mark>可以认为强化学习包含了全部人工智能</mark>**”（Reinforcement learning might be considered to encompass all of AI）。

## 8.5 计算向自然学习 vs. 自然向计算学习

以色列海法大学的进化生物学家 Livnat 和伯克利的理论计算机科学家 Papadimitriou
2016 年发表了一篇文章“性作为算法”（**<mark><code>Sex as an Algorithm</code></mark>**），引起轰动。

* 喜欢的人认为这为进化论找到了新视角，而不喜欢的人则批评杂志的编者和作者是为了博眼球。
* 这篇文章质疑了性在进化中的作用。
* 哈佛大学的理论计算机科学家、图灵奖获得者 Leslie Valiant 曾经从计算的角度研究过机器学习和进化，他把进化当作学习的特例。Livnat 和 Papadimitriou
  认为**<mark>有性繁殖不太容易达到最优点，而无性繁殖才更像是优化算法</mark>**，
  他们**<mark>把遗传算法比作有性繁殖，模拟退火算法比作无性繁殖</mark>**。

如果说遗传算法是**<mark>微观地向生物内部机制学习</mark>**的话，强化学习则是更为**<mark>宏观地向自然学习</mark>**。

## 8.6 生物学激发的学科都缺乏计算理论的基础

**<mark>无论是遗传算法、深度学习还是强化学习，都缺乏计算理论的基础</mark>**。

* 生物学激发的学科都是**<mark>模拟自然，它们都不需要解释，不需要了解内部原理</mark>**，而只要能查看输出结果就够了。
* 数学大概是所有学科中离生物学最远的学科。

## 8.7 参考资料

### 整体大于局部之和：涌现（emergence）现象

Holland (1975)是遗传算法的原创著作。

Holland 曾经写过几本科普读物，但大科学家未必是好的科普作家，他的著作不适合完全的门外汉。
另外，他的哲学观点是整体论的，**<mark>他认为整体大于局部之和</mark>**，大量的“局部” 凑到一起，可以形成**<mark>“涌现”</mark>** （emergence）现象。

### Sutton and Barto  (1998) 强化学习的原创著作

Sutton and Barto  (1998) 是强化学习的原创著作，在网上可免费获取。

强化学习的教科书里最爱用的 Q-learning，是 Chris  Watkins  1989 年在他的剑桥博士论文里提出的。

### 科普文章：“谁能说出更大的数”

理论计算机科学家 Scott Aaronson 曾经写过一篇非常有意思的科普文章“谁能说出更大的数”（**<mark><code>Who Can Name the Bigger Number</code></mark>**），这可以是算法信息论的入门。

# 9 哲学家和人工智能 

> The real discovery is the one that makes me capable of stopping doing philosophy when I want to, the one that gives philosophy peace. 
> ——Wittgenstein（维特根斯坦）

## 9.1 两类哲学家：深刻的和混饭的

**<mark>哲学家不一定懂哲学</mark>**，就像相声演员不一定会说相声，**<mark>这是低门槛行业的通病</mark>**。

《计算机不能干什么》，1965 是对《炼金术与人工智能》的扩充，对人工智能的全面批评。

哲学家有两类，**<mark>一类是深刻的，一类是混饭的</mark>**。

* 罗素和弗里格是深刻的，没有他们，就不会有数理逻辑，也就不会有哥德尔、丘奇、图灵，以及后来的计算机科学。
* 但没有现代的欧陆哲学，世界不过省了些粮食而已。

没有胡塞尔和海德格尔，Minsky 照样会想出“框架” ，从而催生后来的“**<mark>面向对象的程序设计</mark>**”方法论。
所谓“顶层 ”概念就是 Java 程序设计语言里的 Object。

按照德雷弗斯们的说法，**<mark>哲学系是不是应该要求</mark>**读现象学的**<mark>博士必须熟练掌握一门面向对象的程序设计语言</mark>**？

在 20 世纪 80 年代末期，神经网络研究复兴之后，德雷弗斯对人工智能的全面批评也缩小为对符号派的专门攻击。
他和他的兄弟斯图亚特·德雷弗斯一起撰文写书。斯图亚特虽然是运筹学专家，但一直都在做神经网络的研究，甚至号称发明了“**<mark>反向传播</mark>**”（back-propagation）的原始概念。

德雷弗斯曾经引用梅洛庞提批判人工智能：**<mark>人脑是和环境直接交流的，而不是通过表示</mark>**（representation）。

## 9.2 塞尔和中文屋

1980 年塞尔在《行为与脑科学》杂志上发表了 **<mark><code>Minds, Brains and Programs</code></mark>** 一文。文中的一个思想实验“中文屋”  马上成为最喜欢被引用的假想实验之一。

### “中文屋”思想实验

“中文屋”思想实验是这样的：

* 假设有个只懂英文不懂中文的人（“我”）被锁在一个房间里，屋里只给“我”留了一本手册或一个计算机程序，
  **<mark>这个手册或程序教“我”在收到中文信息时如何用中文应对</mark>**。
* 屋外的人用中文问问题，屋里的“我”**<mark>依靠程序用中文回答问题</mark>**，沟通方式是递纸条。

塞尔的问题是：**<mark>如果屋外的人不能区分屋里的人是不是母语为中文，那么屋里的“我”是不是就算懂中文</mark>**？

**<mark>塞尔自己认为“我” 不懂中文</mark>**。很明显，这个场景源自图灵测试，只不过图灵测试的环境是英文，而中文屋里既有中文又有英文。

### 解读

塞尔的文章出来后，引起轰动。其实轰动的原因很简单：**<mark>谈论这种玩意儿没什么门槛，谁都可以说三道四</mark>**：哲学家、科学家，以及各种媒体人。

塞尔毕竟是老练的哲学家，已经预测大家会质疑他的论断，他在文尾也设想了各种回答。

* 第一个问题是，我们**<mark>只是算屋里人理解中文呢，还是屋子加人作为一个系统理解中文</mark>**。
  塞尔的论断是屋里人即使查遍手册，顶多算是理解语法，而不算理解语义。
* 我们可以问塞尔这样的问题：**<mark>一个坐飞机的人算能飞吗</mark>**？如果对这些问题的答案都是“算” ，那中文屋作为一个系统为什么不算理解中文呢？


塞尔认为必须内化（换句话说：手册必须变成人身的一部分）才能算懂中文，那么内化到什么程度才能算呢？

* 爱因斯坦说“**<mark>我的笔加上我要比我自己聪明</mark>**”，笔算不算外化？
* 内化是完全的物理隐藏，还是只是个反应时间问题？在一开始查手册时，反应时间必定很慢，但熟能生巧之后，查手册变成下意识的动作，那算内化吗？
* 内化和辅助工具的大小也有关系。如果语音识别工具是桌面电脑，我们可能不会认为对话中的两个人理解了对方的语言。
  但如果**<mark>这个工具可以微型化，直接内化到耳朵里</mark>**，那算不算理解？

### 反“强人工智能”

塞尔认为他不是反人工智能，他只是反“强人工智能”。

中文屋**<mark>测试的不是屋中的“我”，而是屋中的程序</mark>**。
如果那本神奇的手册或者程序已经通过图灵测试，那程序就是一个机器翻译的神器。这本身就是强人工智能了。
而且那程序已经有语义功能了。

**<mark>假设游戏不是中文翻译，而是下棋，那 “我” 算不算会下棋</mark>**？
断言中文屋是不是有智能，就像断言 AlphaGo 会不会下围棋一样，要看应用场景。

## 9.3 普特南和缸中脑

### 思想实验：缸中脑

1981 年普特南出版了《理性、真理与历史》（**<mark><code>Reason, Truth, and History</code></mark>**）一书，
该书的开篇就给出了“缸中脑”的假想实验。

<p align="center"><img src="/assets/img/brief-history-of-ai-zh/Braininvat.svg.png" width="50%" height="50%"></p>

> In philosophy, the brain in a vat (BIV) is a scenario used in a variety of
> thought experiments intended to draw out certain features of human
> conceptions of knowledge, reality, truth, mind, consciousness, and meaning.
>
> Wikepedia [Brain in a vat](https://en.wikipedia.org/wiki/Brain_in_a_vat):

普特南更进一步设想，假设所有的感觉器官都泡在缸里，而外面的世界就是一台大自动机。

缸中脑知道如何与外部世界做对应吗？**<mark>泡在缸中的人脑，如何知道自己是颅中脑，还是缸中脑</mark>**？

人工智能的基本问题是可否造一台机器能有智能，
“缸中脑”中的机器则起了另一种作用：**<mark>人脑是否能确定外在的世界是直接实在还是间接实在</mark>**。

### 《黑客帝国》、《盗梦空间》

**<mark>科幻电影《黑客帝国》（Matrix）、《盗梦空间》（Inception）等都受“缸中脑”思想实验的启发</mark>**。

## 9.4 给哲学家一点忠告

### 哲学指导科学？

**<mark>曾经有一个教条：哲学指导科学</mark>**。费曼、惠勒和杨振宁等物理学家都曾撰文批驳。
但这恰是德雷弗斯的立场。维特根斯坦曾经有言：**<mark>哲学家的工作应该是一直给人提醒（assembling reminders），而不是指导</mark>**。

### 哲学空洞化

偏重科学和逻辑的**<mark>英美分析哲学也挡不住哲学的颓势，最后一个从哲学中脱离的硬学问是逻辑</mark>**，
目前**<mark>最好的逻辑学家都在数学系和计算机系，哲学已经空洞化</mark>**。

如果真认为海德格尔有用，就应该像弗里格和罗素清理逻辑那样，
**<mark>把这些东西整理成可以交流的形式。
也许哲学家真怕他们惯用的冷僻词汇被翻译成通俗易懂的语言</mark>**。
当代哲学，尤其是欧陆哲学，就像韩国整容术，乍一看唬人，其实遗传不了。

### 整个人工智能就是个大的假想实验

彭罗斯曾经这样谈到机器的情感和道德：**<mark>如果你买一台计算机，它是有情感的，那么我们就有道德问题，因为计算机的意愿可能被违反</mark>**，并可能会被当作奴隶。
我们首先必须说道德是一个社会问题，也就是说当一个社会只有一个个体（无论是人还是计算机）时，是不存在道德问题的。

丹尼特曾说哲学家喜欢假想实验。其实从某种意义上说，**<mark>整个人工智能就是个大的假想实验</mark>**。
只不过哲学家用纸和笔，而计算机科学家用计算机硬件和软件。
本质是一样的。
不同的是哲学家从不为假想实验的结果所苦恼，反而会时不时洋洋自得；而计算机科学家则偶尔会被他们取得的成果所惊到。

# 10 人是机器吗？——人工智能的计算理论基础

> humans are nothing but meat machines that carry a computer in their head. —— Marvin Minsky

## 10.1 人是不是机器？

* 认为人是机器的，道理很简单：人也是由各种物理化学机制构成的，当然是机器了。
  早有法国哲学家美特里，现有 **<mark>DNA 双螺旋结构发现者克里克</mark>**，都持这种观点。克里克认为在不远的将来，生命可以在试管中合成。
* 认为人不是机器的，论据是人有很多功能，目前机器无法完成，尤其是那个叫“灵魂”  的神奇东西。

### 《论可计算的数》和图灵机的定义

计算机科学起源于图灵 **<mark><code>1936</code></mark>**
年那篇无论怎么夸赞都不过分的文章“论可计算的数”，这是人类文明最重要的成果之一。
图灵在这篇文章中定义了后来被他的导师丘奇称为“图灵机”的计算装置：

* 一条无穷长的纸带，
* 一个读写头在一个控制装置的控制下在纸带上方左移右移，读取纸带上的内容并在纸带上写 0 或 1。

图灵的初衷是让他的机器模仿人类计算者。

### 同源问题和相关问题

“人是机器吗”这个问题有**<mark>很多同源的古老哲学问题</mark>**，例如，“心-脑”（mind-brain）和“心-身”（mind-body）。
**<mark>还有很多相关问题</mark>**，例如，自由意志和自我意识。

### 如果人是机器，那是模拟机器还是数字机器？

* **<mark>按照冯诺伊曼的说法，神经系统的本质是数字的</mark>**，尽管构成神经系统的化学和生物过程的描述可能是模拟的。
* **<mark>现代物理学的一个假设是整个宇宙都是离散的，也即数字的</mark>**。
* 人工智能符号派的基础之一是所谓“物理符号假设”，这个假设要求计算装置必须是数字的，或者说变量必须是离散的。
* 费曼就曾说世界是数字的。

如果机器是数字的，那么图灵机就是简单又有力的模型。
**<mark>对于离散的量，二进制就足够了</mark>**。

**<mark>朴素唯物主义认为世界是连续可分的</mark>**，从某种宏观的意义上说，**<mark>朴素唯物主义是经典物理的思想基础</mark>**。
**<mark>历史问题有点像海岸线问题，尺度不同则结论也不同</mark>**。
新的量子物理认为世界是离散的、有限的。

## 10.2 Church-Turing Thesis：为什么图灵机是最重要的发明？

在人类发明的所有计算装置中，图灵机是直觉上最简单最可靠的。

在计算理论里，有一个著名的丘奇图灵论题（Church-Turing Thesis）：
**<mark>所有功能足够强的计算装置的计算能力都等价于图灵机</mark>**。这是一个观察，而不是定理。

### 通用图灵机和冯诺依曼架构

图灵在发明图灵机时，还定义了 Universal Turing Machine，简称 UTM，译为“广义图灵机/万能图灵机/通用图灵机”。

* UTM 的核心思想就是**<mark>一个图灵机的执行过程也可被编码成数据，放到纸带上</mark>**，因此一个图灵机可以通过执行纸带上的程序来模仿另一个图灵机的行为。
  这台能模仿其他图灵机的图灵机就成了**<mark>通用图灵机</mark>**。
* 这是一个很深刻的思想，**<mark>现在的软件产业都得益于此</mark>**：被编码的图灵机就是软件。
* 后来冯诺伊曼设计的计算机被人称为冯诺伊曼架构，其最核心的思想就是存储程序（Stored Program）。这个思想其实就是来自万能图灵机：被编码的图灵机就是存储的程序。

### 纯逻辑或数学的东西联系到物理世界：函数 -> 纸带和读写头

**<mark>冯诺伊曼把计算机的所有原创思想的功劳都给了图灵</mark>**，并批评那些对图灵机实际意义缺乏认识的人。

**<mark>有了图灵机，我们就很容易把原来是纯逻辑或纯数学的东西</mark>**（例如递归函数和λ演算等）
**<mark>和物理世界联系起来了，函数成了纸带和读写头</mark>**。

## 10.3 不可能存在比图灵机更强的计算装置

Church-Turing Thesis 的一个自然结果就是，不可能存在比图灵机更强的计算装置。

* 20 世纪 80 年代初就有人证明**<mark>三层以上的神经网络可以逼近任意连续函数</mark>**。
* 80 年代末期，Steve Judd 证明**<mark>三层以上的神经网络学习问题在图灵机上是 NP 完全的</mark>**。
* 本书作者证明了在 BSS 模型上，类似的神经网络学习问题等价于线性规划问题。

**<mark>目前各种神经网络</mark>**学习算法**<mark>都是工程，鲜有科学</mark>**，
神经网络算法多是些经验算法外加调参数，从业人员也多数没有计算理论的训练。
伴随暴发户和显学的必然是浮躁之气。在各种学习算法里，很少看到目前关于什么算法适合什么问题的理论指导。

## 10.4 BBS 实数模型

BSS 模型的一个很大假设是，**<mark>任意精度的实数四则运算可在单位时间内完成</mark>**，
这在数值分析中是有用而又方便的假设，但目前尚不知道如何在物理上实现。

其实即使在数值分析之外，我们经常做类似的假设，例如，在排序算法分析中，任意精度的数（可能是实数）之间的比较是单位时间的。

在 BSS 中，一阶逻辑的所有东西都是可判定的。这和图灵机是截然不同的，图灵机停机问题就是不可判定的。
BSS 和图灵机的这个本质区别可溯源到 20 世纪 30 年代初期。
那时哥德尔证明了整数的一阶逻辑是不可判定的。
但几乎在同时，塔尔斯基证明了实数的一阶理论（几何和代数）则是可判定的。我们可以说图灵机和 BSS 分别是哥德尔定理和塔尔斯基定理的计算体现。

有些复杂性的性质，BSS 也和图灵机不同。比如线性规划在图灵机上被证明是多项式时间的，但在 BSS 上，复杂度是啥，目前不知道。
如果在 BSS 上可以找到线性规划的多项式时间的话，在图灵机上就可以找到强多项式时间算法。这个问题被斯梅尔称为最重要的计算机科学的理论问题。

按照费曼的说法，宇宙是数字的，换句话说，宇宙不是连续的实数，空间是一种网络，而时间也不是连续的。

## 10.5 量子计算

《费曼计算机科学讲义》

IBM 是计算物理学的源头。计算的物理学研究有实际需求。

### 图灵机的物理约束

从计算的角度看，图灵机只有数学约束而没有物理约束。

从真实世界看，一个可能的**<mark>物理约束是能量</mark>**：图灵机的读写头和纸带的运动是需要能量的。

### 逻辑运算与能量的关系

现代计算机的组件是逻辑门，有两种门，

* 可逆的，如“非门”；
* 不可逆的，如“与门”。

IBM 的物理学家朗道尔（Rolf Landauer）在 1961 年提出了朗道尔原理：任何**<mark>不可逆计算都需要能量</mark>**。

同在 IBM 的另一位物理学家本内特（Charles  Bennett）在 20 世纪 70 年代提出**<mark>可逆运算不需要能量</mark>**，
并证明对任何图灵机都能找到一个对应的可逆版本，能实现同样功能而不损失效率。

### 量子计算机：（在对的时刻）测量而非（一步步）计算

费曼考虑的问题是如何以任意精度来模拟一个物理系统。
他的方法是构造一台量子计算机，它**<mark>求解问题的时间不随问题的规模呈指数增长</mark>**。

量子计算并**<mark>不是一步一步的经典计算，而只是测量系统的输出结果</mark>**。

费曼认为**<mark>测量本身也是一种计算</mark>**。

* 当计算量很大时，最简单的方式是让自然界自己该干啥干啥，而在对的时刻测测结果就可以了。

### 举例：子弹的弹道，生成随机数

举一个不精当的比喻，想知道子弹的弹道，两种方式，

1. 考虑所有可能外部内部因素，依靠计算；
2. **<mark>让子弹飞，然后测量</mark>**。

随机数可以通过伪随机函数生成，也可以通过测量一些噪声源得到。
图灵 1949 年就研究过通过外部电子噪声源得到随机数的方法。

在图灵机上很难求解的问题有可能在量子计算机上用**<mark>多项式时间</mark>**解决。其中最热门的问题是素数分解。

## 10.6 计算理论的哲学寓意

### 神经网络研究者数学和计算理论功底的缺乏

人们常说是 Minsky 和佩珀特的《感知机》（Perceptrons）一书导致了神经网络研究近 20 年的衰败，
但神经网络的研究者不该**<mark>反省下自己数学和计算理论功底的缺乏</mark>**？

从当下人工智能的浮夸风气中，没看出吸取了什么教训。

### Donald Knuth：量子力学为自由意志提供了空间，也使得上帝可以操纵世界而不违反物理定律

Donald Knuth（计算机科学家中位数不多的有神论者） 说**<mark>量子力学为自由意志提供了空间，也使得上帝可以操纵世界而不违反物理定律</mark>**。

我很少看到计算机科学家敢对物理学家说三道四，姚期智大概是唯一的例外。

# 11 智能的进化

> **<mark><code>Science is what we understand well enough to explain to a computer. Art is everything else</code></mark>** we do. —— Donald Knuth

## 11.1 Human Advantage: How Our Brains Became Remarkable

* 畅销书，并被翻译为多种语言。2017 年该书中文版以《最强大脑》为题出版。
* 创造的“大脑汤”（brain soup）的方法最终使她成功地测定不同动物大脑的神经元数量。
* 书中不仅有研究成果，还有更有意思的研究过程，包括她是如何把大象的大脑从非洲弄到美洲的新奇故事。

### 脑结构和神经元数量

不同动物的脑构造有所不同，脑中的神经元数量也完全不同，

* 人脑中总共有 860 亿个神经元（用 LLM 术语来说就是 **<mark><code>86B</code></mark>**），其中大脑皮层有 160 亿个神经元（**<mark><code>16B</code></mark>**）。
  **<mark>大脑皮层的神经元数量决定了动物的智力水平</mark>**，人的大脑皮层中神经元数量远高于其他物种，所以人类比其他物种更聪明。
* 大象的脑子总共有 2570 亿个神经元，但是其中 98% 的神经元都存在于小脑中。大脑皮层只有 56 亿个神经元，无法与人类相比。

### 神经元数量越多，能耗也越大

**<mark>大脑皮层中的神经元数量越多，能耗也越大</mark>**。

* **<mark>人脑每天消耗的能量占人体全部耗能的 25%</mark>**。
  人之所以能够很快超越其他物种，主要是因为人类掌握了烹饪技术。能够在短时间内摄入大量卡路里以支持大脑运转。
* 其他物种则将摄入的卡路里用于维持身体运转，不得不牺牲大脑皮层的神经元数量。

### 用不同的时间粒度看待过去，会得到不同的结论

* 《尤利西斯》中的几个小时，茨威格作品中人物的一生，或赫拉利的七万年，关心不同的过程。
* 粒度也可以是主体的，一个基因，一个人，一个群体，不一定非得是一个小的物质颗粒只配得上小的时间单位。
* 想想基因人类学，基因在几万年的空间分布，帮我们了解人类的起源和迁移。
* 当用太大的颗粒度研究历史时，历史学家的用处会令人质疑。

## 11.2 机器：从代替人的体力到代替人的智力

过去的机器旨在节省人的**<mark>体力</mark>**，现在的机器开始代替人的**<mark>智力</mark>**。

### 人作为物种，不再具备进化的竞争优势？

人通过两性繁殖的进化速度远远赶不上机器。

* 机器的进化速度服从摩尔定律——每 18 个月性能提升一倍，而人的进化速度则是 20 年一代人。
* 人作为物种，是不是不再具备进化的竞争优势？
* 依靠硬件的摩尔定律，是不是可以达到超级智能？

### 新的智能形态：Agent？

新的智能存在可以是人工智能的 **<mark><code>agent</code></mark>**，也可以是生物学意义上的物种。

## 11.3 基因修复的伦理问题

通过修复一个受精卵的一小段染色体，就可以避免或治疗某种疾病。这是一个真实的伦理问题，因为已经有这样的病例发生。

* 如果孩子出生，那么他/她的父母是谁？
* 多小算是“一小段”，1% 还是 49%？
* 更进一步：可不可以有更多不同来源的基因参与？
* 英国《经济学人》2017 年 2 月的一期封面标题就是“Sex and Science”

## 11.4 机器人三定律之一：机器不能伤害人

维纳曾经说：“**<mark>我们最好能够确认，我们给机器设定的目标确实是我们想要的</mark>**。”

物理学家改行的科幻作家阿西莫夫曾提出机器人三定律，第一条就是**<mark>机器不能伤害人</mark>**，
但“什么是伤害”本身就不好定义。AlphaGo 战胜李世石和柯洁，算是对他们的伤害吗？

# 12 当我们谈论生死时，我们在谈论什么？

> **<mark><code>I don’t want to achieve immortality through my work; I want to achieve immortality through not dying</code></mark>**. —— Woody Allen（伍迪·艾伦）

## 12.1 苏格拉底之死和《斐多篇》

苏格拉底说：哲学家只研究“正在死”（dying）和“刚刚死”（being dead）。除了这个啥都不管。

苏格拉底因为三项罪名被判死刑：腐蚀雅典青年，不敬城邦和引入自己的新神。
受审前一天恰好赶上雅典的“花船节”，祭祀的船要离开雅典再返航。花期，城邦要保持清洁，因而不能执行死刑，于是苏格拉底临死前有一段时间可以和学生们聊哲学。
柏拉图据此写了四篇对话。

**<mark>耶稣之死和苏格拉底之死不同，耶稣完成了使命，苏格拉底留下了一堆问题</mark>**。

他说人**<mark>追求真理的最大束缚就是肉体</mark>**，为了得到终极智慧，灵魂**<mark>必须超越肉体，也就是摆脱感官的限制</mark>**。
换句话说就是人必有一死。他最后一天的谈话被当时的在场者斐多记录，最终变成了柏拉图的**<mark>《斐多篇》</mark>**。

## 12.2 作者和苏格拉底之间的假想对话

挺有意思的一段哲学对话，关于**<mark>“永生”</mark>**，这里就不放了，
感兴趣可以网上搜搜，或者读完这份笔记觉得这本书不错，买本电子/纸质书支持下作者。

# 13 总结

|  | 逻辑派/规则派/符号派 | 统计派 |
|:-----|:-----|:-----|
| 哲学层面 | **<mark>理性主义者</mark>** | 经验主义者 |
| 经济方式类比 | 计划经济| 自由市场经济|
| 视角和可解释性 | **<mark>上帝视角，第三人称叙事，更具可解释性</mark>** | 第一人称叙事，不可解释性（e.g 深度学习）令人困扰 |
| 科学史角度 | 还原论（reductionism） | **<mark>涌现论</mark>**（emergentism）|

科学史对科学也有还原论（reductionism）和涌现论（emergentism）之分，规则派接近还原论，统计派可以算作涌现论。

如果说**<mark>英美分析哲学的工具支撑是逻辑</mark>**的话，那么在某种意义上，**<mark>博弈论可被当作实用主义的新工具，博弈论涉及 Multi-Agent</mark>**。
我并没有非得把自然派附会到实用主义的意思。曾经被认为是复杂的统计派问题，例如图像处理和语音识别，现在已经得到解决或者至少已有解决的思路。

# 附录

## 附录 1：图灵小传

<p align="center"><img src="/assets/img/brief-history-of-ai-zh/Alan_Turing_Sackville_Gardens.jpg" width="60%" height="60%"></p>

曼彻斯特的公园里，图灵雕像的底座，引用了罗素的话：“数学不仅有真理，也有最高的美，那是一种冷艳和简朴的美，就像雕塑。”

> **<mark><code>Mathematics, rightly viewed, possesses not only truth, but supreme beauty — a
> beauty cold and austere, like that of sculpture</code></mark>**, without appeal to any part
> of our weaker nature, without the gorgeous trappings of painting or music,
> yet sublimely pure, and capable of a stern perfection such as only the
> greatest art can show. The true spirit of delight, the exaltation, the sense
> of being more than Man, which is the touchstone of the highest excellence, is
> to be found in mathematics as surely as poetry.
>
> 伯特兰·罗素，《西方哲学史》

## 附录 2：人工智能前史：图灵与人工智能

图灵 1950 年在英国哲学杂志 Mind 上发表文章“计算机与智能”，文中提出“模仿游戏”，被后人称为“图灵测试”。

* 这篇文章被广泛认为是机器智能最早的系统化科学化论述。
* 但图灵在 **<mark><code>1941</code></mark>** 年战时就开始思考机器与智能的问题，1947 年图灵在伦敦皇家天文学会就机器智能发表演讲。1948 年图灵把这次演讲整理成文章，题为“智能机器”（“Intelligent Machinery”），作为英国国家物理实验室（NPL）的内部报告，但没有公开发表。
* 这篇文章迟至 1969 年才在年刊型论文集《机器智能》上发表。但由于和 1950 年文章的题目类似，并没有引起人们的重视。

1948 年的文章对智能的概念采取了更宽泛的说法，**<mark>图灵探讨了大脑皮层</mark>**，

* 他认为婴儿的大脑皮层是非组织的（unorganised）。
* 在图灵的用语里，**<mark>“非组织”就是“通用”的意思，发育的过程就是组织化的过程</mark>**。
* 他指出人身上的任何小部件都可以用机器来模仿，他**<mark>还提到基因、进化和选择</mark>**。

正是因为如此，麻省理工学院的机器人专家布鲁克斯认为图灵（1948）是人工智能两条路线分歧的原点，
而他自己的观点则是图灵 1948 年的文章比 1950 年的更为重要。
图灵 1948 年的文章提到了 **<mark><code>embodied intelligence</code></mark>** 和 disembodied intelligence 的区分。

图灵进一步**<mark>预测到 2000 年，机器内存会达到 1GB</mark>**（预测这么准还真挺神）。

这篇文章为后来的一系列后学者模仿的文章提供了范文的效果，例如塞尔的“中文屋”和普特南的“缸中脑”。

## 附录 3：冯诺依曼与人工智能

> Talent hits a target no one else can **<mark><code>hit</code></mark>**;
> Genius hits a target no one else can **<mark><code>see</code></mark>**.
> —— Schopenhauer（叔本华）

冯诺伊曼被引用最多的话是：“我们应该预测所有稳定的过程，控制不稳定的过程。”
（**<mark><code>All stable processes we shall predict. All unstable processes we shall control.</code></mark>**）
其实这并非是老冯的原话，而是弗里曼·戴森转述老冯 1950 年在普林斯顿的讲座的精神，那时他是多么自信啊。

## 附录 4：计算机与智能，turing paper

建议参考翻译，**<mark>阅读图灵的原 paper</mark>**。

# 后记

本书的写法比较偏重基础和方法论，而不太注重应用。

费曼在加州理工学院教书时，学期的最后一节课都是请学生问问题，只要不涉及政治、宗教和期末考试，什么问题都可以问。

本书也参考这一方式，回答读者几个问题：

* 问：这次的人工智能是泡沫吗？
* 答：**<mark>人工智能和人们关心的某些终极问题有关，这些问题过去是哲学家和科幻作家的地盘</mark>**，
  **<mark>计算机科学为人们提供了用科学和工程的手段回答这些问题的方法，旁人自然会对这些方法存在过高的期望</mark>**，过高的期望自然也会带来过高的投资。
  泡沫的破裂就是投资的失败。比人工智能更年轻的互联网，起伏的周期更短。从投资的角度看，某些特定的人工智能应用领域确实存在过热现象。

* 问：算法、数据和算力，哪一项对这次人工智能的复兴贡献最大？
* 答：我正在对这个问题做一项定量的研究，但目前还没有确定性的结果。要我猜的话，**<mark>贡献排序应该是：算力、数据和算法</mark>**。
  没有足够的算力，就没有办法处理海量数据，很多算法的精化是以某些特定的硬件为前提的。
  **<mark>算力的提升恰好到了一个临界点，使得各种学习算法成为可能</mark>**。

----

<a href="https://notbyai.fyi"><img src="/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg" alt="Written by Human, Not by AI"></a>
<a href="https://notbyai.fyi"><img src="/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg" alt="Written by Human, Not by AI"></a>
