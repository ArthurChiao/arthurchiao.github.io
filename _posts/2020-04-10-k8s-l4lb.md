---
layout    : post
title     : "L4LB for Kubernetes: Theory and Practice with Cilium+BGP+ECMP"
date      : 2020-04-10
lastupdate: 2020-04-10
author    : ArthurChiao
categories: cilium bgp ecmp k8s
---

As networking engineers that responsible for on-premises Kubernetes clusters,
you may face the need to **expose your services inside a Kubernetes cluster to
the outside world** (e.g. another Kubernetes cluster).  As show in Fig 0.1.

<p align="center"><img src="/assets/img/k8s-l4lb/exposing-k8s-service.png" width="90%" height="90%"></p>
<p align="center">Fig. 0.1 Exposing a Kubernetes service to outside world</p>

Kubernetes provides two **models** for this scenario:

* [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/): for exposing services via layer 7
* [LoadBalancer](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/): for exposing services via layer 4

but **leaves the implementations to each vendor**. For example, if you are on
AWS, it provides [ALB](https://aws.amazon.com/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/)
and [ELB](https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws), each for layer 7 and layer 4.

This post reasons about the latter one, and provides a simple implementation
by combining open source softwares.

## 1. Problem Definition

Suppose you deployed several **DNS servers** (pods) inside a Kubernetes
cluster, intended for providing **corp-wide** DNS service.

Now the question is: **how would your legacy applications** (e.g. running in
bare metal or VM) or **applications in other Kubernetes clusters access this DNS
service**? As depicted in Fig 1.1:

<p align="center"><img src="/assets/img/k8s-l4lb/exposing-dns-service.png" width="90%" height="90%"></p>
<p align="center">Fig. 1.1 Exposing DNS service in a Kubernetes cluster to outside world</p>

Some reminders:

1. DNS provides service on **UDP** (or TCP) **port `53`** (thus **layer 4**).
2. DNS pod is **stateless**, here "stateless" means all instances are identical
   and active, rather than active-backup or master-slave.

# 2. Requirement Analysis

## 2.1 L4LB Model

At first glance, it seems that classic layer 4 load balancer (L4LB) model could
be used to address this problem, that is:

1. assign a VIP to the DNS cluster, all clients access the service by VIP
1. forward traffic from VIP to a specific backend with load balancing algorithms

<p align="center"><img src="/assets/img/k8s-l4lb/classic-l4lb.png" width="35%" height="35%"></p>
<p align="center">Fig. 2.1 Classic L4LB</p>

## 2.2 Special Considerations

Well, this's on the right direction, but lacks some important aspects.

First, **our backends (Kubernetes pods) are very dynamic and subject to frequent
changes**: kinds of failures
(or internal scheduling) may trigger Kubernetes to kill or reschedule a pod.
This may happen frequently;

Secondly, the **IP address of a Pod is not immutable**. In comparison,
traditional L4LBs favor fixed IP backends, they just pull in or pull out
existing backends by health check status.

Thirdly, **backends may be scaled up/down according to capacity plans**, these
backend changes should be timely reflected on L4LB, and be transparent to your
users/clients without human interventions (some kind of service discovery).

This makes the L4LB a very unique position:

* on the one hand, it must be reachable from the outside world (while most
  Kubernetes resources couldn't be)
* on the other hand, it belongs to a Kubernetes cluster - as it could/must
  listen to Kubernetes changes in order to update its forwarding rules (from VIP
  to backends)

## 2.3 Technical Requirements Summary

Now we could summarize the above analysis into following **technical needs**:

1. **Persistent/invariant L4 entrypoint for accessing a Kubernetes service from
   the outside world**: the entrypoint, e.g. VIP, should be unique and invariant,
   hiding users/clients from backend changes from.
2. **Load balancing**: load balance requests between different backends.
3. **Reactive to backend changes**: update forwarding rules according to backend
   changes, e.g. instances scale up/down.

For production ready, there also involves the 4th requirement:

\ 4. **High availability**: both software and hardware (if there is).

In this article, we will show such a (simple) design/implementation for
on-premises bare metal kubernetes clusters.

# 3. An L4LB Solution

This post assumes the underlying physical network uses spine-leaf architecture
(physical network topology only affects ECMP in this post).

<p align="center"><img src="/assets/img/k8s-l4lb/l4lb-topo.png" width="85%" height="85%"></p>
<p align="center">Fig. 3.1 Topology of our L4LB solution</p>

The overall design is shown in Fig 3.1:

1. Dedicate several nodes as **L4LB nodes**.
2. Running a **BGP agent** on each L4LB node, announcing a specific CIDR to
   underlying network, e.g. `10.1.0.0/24`. IPs in this CIDR will be used as
   VIPs, also called ExternalIPs in K8s.
3. Running a **Cilium agent** on each L4LB node, which listens to Kubernetes
   resources (especially ExternalIP Services and Pods), and generates BPF rules
   for forwarding packets to backend pods.
4. **Glue VIP CIDR and Cilium agent in the kernel** with a
   [dummy device](https://unix.stackexchange.com/questions/530700/what-is-the-use-of-the-dummy-device)
   on each L4LB node.
5. **Enable ECMP** on physical networks.

Traffic path when accessing the example service from the outside world:

<p align="center"><img src="/assets/img/k8s-l4lb/l4lb-traffic-path.png" width="85%" height="85%"></p>
<p align="center">Fig. 3.2 Traffic path when accessing example service from outside of Kubernetes cluster</p>

where the four numbered steps in the graph:

1. Client packets -> physical network (spine)
2. Spine -> L4LB node, via ECMP
3. In-kernel routing (glue layer)
4. L4LB -> Backend pods, with Cilium load balancing

Details explained below. Let's see how it meets the 4 requirements listed in
Section 2.3.

## 3.1. BIRD - BGP agent

L4LB nodes run [`bird`](https://bird.network.cz/) as BGP agent.

When bird announces a CIDR (e.g.  `10.1.0.0/24`) to the data center network, all
subsequent packets with destination IPs within this CIDR  (e.g.  `10.1.0.2`)
will be routed to this node.

If not familiar with BGP announcement, you could simply think of it as
broadcasting something like `"CIDR 10.1.0.0/24 is at host XXX"` to the physical
network. When routers receive this message, they will install it into their
routing tables.

This solves the `1st requirement`: **persistent/invariant (L4) entrypoint**.

## 3.2. Cilium - Networking agent

[`cilium-agent`](https://github.com/cilium/cilium) on L4LB node will listen to
Kubernetes apiserver, and generate BPF rules for Kubernetes ExternalIP services
to forward traffic from VIPs (which are held by L4LB nodes) to backend pods.

This solves the `2nd and 3rd requirement`: **load balancing** and **timely
react to backend changes**.

## 3.3. ECMP - Physical link HA

With ECMP (Equal Cost Multiple Path) enabled, we could have multiple L4LB nodes
announcing the same CIDR, HW routers will load balancing those packets to our
L4LB nodes (glowing links in Fig 3.1 & Fig 3.2).

Besides, there is another BGP optimization option called [Bidirectional Forwarding
Detection](https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-16/irg-xe-16-book/bgp-support-for-bfd.html)
(BFD), enable it between
routers and `bird` will accelerate routes convergence inside the entire network.

Thus, we solved the `4th requirement`: **high availability**.

## 3.4 Glue In-kernel Processing

Till now, packets with destination IPs within CIDR (VIPs) will arrive our L4LB
nodes, and `cilium-agent` will generate forwarding rules for those VIPs.

But one part is still missing: **there are no rules in the kernel to redirect
those packets into `cilium-agent`'s processing scope**. So without additional work,
those packets will be dropped inside kernel, instead of be forwarded to backends
by Cilium BPF rules, as shown in Fig 3.3:

<p align="center"><img src="/assets/img/k8s-l4lb/no-dummy.png" width="60%" height="60%"></p>
<p align="center">Fig. 3.3 Packets dropped in kernel due to no routing rules</p>

So we need some glue work to fill the gap. Our solution is
**create a dummy device, which holds the first IP in our CIDR**.

<p align="center"><img src="/assets/img/k8s-l4lb/add-dummy.png" width="60%" height="60%"></p>
<p align="center">Fig. 3.4 Traffic process in kernel after add dummy device</p>

# 4 Configurations

System info:

1. Centos: `7.2`
1. Kernel: `4.18+`
2. BIRD: `2.x`
3. Cilium: `1.7+`

Beside, this post assume the Kubernetes cluster uses **direct routing**
for Pod networking. (While adjust Cilium configurations will also make this
scheme work for non-direct-routing cases, that's beyond the scope of this post).

## 4.1 Create and configure dummy device

Install:

```shell
$ modprobe --ignore-install dummy
$ ip link set name cilium-ext0 dev dummy0
```

Configure it to survive host reboot:

```shell
$ cat ifcfg-cilium-ext0
DEVICE=cilium-ext0
IPADDR=10.1.0.1
NETMASK=255.255.255.0
ONBOOT=yes
TYPE=Ethernet
NM_CONTROLLED=no

$ cat ./modprobe.d/dummy.conf
install dummy /sbin/modprobe --ignore-install dummy; ip link set name cilium-ext0 dev dummy0

$ cat ./modules-load.d/dummy.conf
# Load dummy.ko at boot
dummy

$ cp ifcfg-cilium-ext0          /etc/sysconfig/network-scripts/
$ cp modprobe.d/dummy.conf      /etc/modprobe.d/dummy.conf
$ cp modules-load.d/dummy.conf  /etc/modules-load.d/dummy.conf
```

Enable changes:

```
$ systemctl restart network

$ ifconfig cilium-ext0
cilium-ext0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
              inet 10.1.0.1  netmask 255.255.255.0  broadcast 10.1.0.255
              ...

$ route -n
Kernel IP routing table
Destination     Gateway     Genmask         Flags Metric Ref    Use Iface
...
10.1.0.1        0.0.0.0     255.255.255.0   U     0      0        0 cilium-ext0
```

## 4.2 bird

This involves BGP configurations for the `bird` software, as well as HW routers.
Configurations may vary a lot according to the BGP schemes you choose, the
latter is beyond the scope of this post. Refer to
some get started docs, such as the one we wrote:
[Cilium documentation: Using BIRD to run BGP](https://docs.cilium.io/en/stable/gettingstarted/bird/).

For `bird`, add following configurations to `/etc/bird.conf`,

```shell
protocol static {
        ipv4;                   # Again, IPv4 channel with default options
        ...
        route 10.1.0.0/24 via "cilium-ext0";
}
```

Restart bird and verify changes are applied:

```shell
$ systemctl restart bird

$ birdc show route
BIRD 2.0.5 ready.
Table master4:
...
10.1.0.0/24         unicast [static1 2020-03-18] * (200)
     dev cilium-ext0
```

## 4.3 cilium-agent

Normal installation according official documents, as long as the agent could
listen to kubernetes apiserver.

## 4.4 ECMP

ECMP needs to be configured on physical routers.

BFD should be configured both on physical routers and `bird`. Refer to 
[Cilium documentation: Using BIRD to run BGP](https://docs.cilium.io/en/stable/gettingstarted/bird/).

If everything is ok, you should see something like this on your routers:

```shell
ROUTER# show ip route 10.1.0.0
...
10.1.0.0/24, ubest/mbest: 2/0
    *via 10.4.1.7, [200/0], 13w6d, bgp-65418, internal, tag 65418
    *via 10.4.1.8, [200/0], 12w4d, bgp-65418, internal, tag 65418
```

## 4.5 Verification

For nothing but laziness, I will use my handy nginx service instead of deploying
a real DNS service - but the effect is much the same, we will verify our scheme
by accessing the nginx from layer 4.

### 4.5.1 On Master Node

On Kubernetes master, create a service with `externalIPs=10.1.0.2`:

```shell
$ cat cilium-smoke.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  externalIPs:
  - 10.1.0.2
  ports:
  - port: 80
    name: cilium-smoke
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cilium-smoke
spec:
  serviceName: "nginx"
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: library/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: cilium-smoke

$ kubectl create -f cilium-smoke.yaml
```

Check our service's information:

```shell
$ kubectl get svc
$ k get svc
NAME    TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)  AGE
...
nginx   LoadBalancer   10.x.x.x     10.1.0.2      80/TCP   2m
```

As can be seen, it is `LoadBalancer` type, and has an external IP `10.1.0.2`.

### 4.5.2 On L4LB Node

Execute following command inside cilium-agent pod:

```shell
$ cilium service list
...

62   10.1.0.2:53    ExternalIPs    1 => 192.168.1.234:53
                                   2 => 192.168.2.172:53
```

> TODO: more BPF info on L4LB node.
>
> `iproute2` on my current L4LB node is too old, which prohibits my further
> investigation. You could refer to my previous post
> [Cilium Network Topology and Traffic Path on AWS]({% link _posts/2019-10-26-cilium-network-topology-on-aws.md %})
> if intersted.

### 4.5.3 On test node

On a node which is outside of Kubernetes cluster, test our externalIP service
with VIP+Port:

```shell
$ telnet 10.1.0.2 80
Trying 10.1.0.2...
Connected to 10.1.0.2.
Escape character is '^]'.
```

Successful!

# 5. More Thoughts

## 5.1 Pros & Cons

Pros:

* Simple, straight forward, ease of understanding
* Ease of management of ExternalIPs: manage CIDR instead of distinct IPs, less BGP announcements
* Ease of security rule managements for BGP filtering

Cons:

* ECMP hard limits: 16 nodes
* All traffic goes through L4LB nodes, make them the potential bottleneck (e.g.
  BW, CPU processing)
* No [session replication](https://devcentral.f5.com/s/articles/sessions-sessions-everywhere)

## 5.2 Decoupling of BGP agent and networking agent

It's important to understand that, in this L4LB scheme, there are no
couplings between BGP agent and host network agent, that is,

* You could transparently replace `bird` with another BGP agent, e.g. `quagga`
  (but you need to concern whether they support the features you would like, e.g. ECMP, BFD).
* You could also transparently replace `cilium-agent` with other networking
  agents for Kubernetes worker nodes, e.g. `kube-proxy`.

## 5.3 The Hidden CLOS Architecture

<p align="center"><img src="/assets/img/k8s-l4lb/l4lb-traffic-path.png" width="70%" height="70%"></p>
<p align="center">Fig. 3.2 Traffic path when accessing example service from outside of Kubernetes cluster</p>

If think of each TOR pair and corresponding L4LB nodes as an integral entity,
Fig 3.2 could be re-depicted as Fig 5.1:

<p align="center"><img src="/assets/img/k8s-l4lb/hidden-clos.png" width="50%" height="50%"></p>
<p align="center">Fig. 5.1 The Hidden CLOS architecture</p>

which is a [CLOS network](https://en.wikipedia.org/wiki/Clos_network).

Interesting!

## 5.4 Ingress vs LoadBalancer

Ingress provides persistent L7 entrypoints for accessing services inside Kubernetes
cluster from outside world. For example,

* App A: `https://<ingress addr>/app-a/`
* App B: `https://<ingress addr>/app-b/`
* App C: `https://<ingress addr>/app-c/`

Have you wondered how to design an Ingress solution? 

If your Ingress is deployed inside Kubernetes cluster, then **itself must have a
persistent L4 entrypoint**, namely the `<ingress addr>` in the above example.

Combining the L4LB solution in this post and something like Istio Gateway, you
will get a workable Ingress.

# 6. Similar Works

## 6.1 MetalLB

[MetalLB](https://metallb.universe.tf/) is similar with this in that it:

1. Announce VIP via BGP
2. Forward/load-balancing to backends with node agent

Difference from the one in this post:

1. BGP agent announces distinct IPs instead of CIDR
2. No dedicated L4LB nodes
2. Forward/load-balancing to backends via kube-proxy (recently supported Cilium)

Problem may faced: more BGP announcements, more routing entries, more
difficult for filtering BGP announcements.

## 6.2 Katran

[Katran](https://github.com/facebookincubator/katran) is a
general purpose L4LB from Facebook, also based on BGP + BPF.

## 6.3 Kube-proxy + BGP

As mentioned in 5.2, replace Cilium with kube-proxy could also achieve this
goal. Differences including:

1. Forwarding rules on L4LB node will be based on iptables or LVS instead of BPF
1. Performance degrades (maybe)

# 7 Summary

This post analyzed the technical requirements of L4LB for Kubernetes
clusters, and realized a simple one based on Cilium+BGP+ECMP.
